{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time\n",
    "import gensim, operator\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = r'C:/Users/tramh/github/Data-Science-Portfolio/Airlines Covid-19/data/Airlines_dedup.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing json file\n",
    "def parse_json_file(json_file):\n",
    "    with open(json_file) as f:\n",
    "        lines = f.readlines()\n",
    "    parsed_json = [json.loads(line) for line in lines]\n",
    "    return parsed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in deduped dataset - clean\n",
    "mydata = parse_json_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stories\n",
    "stories = [feed['text'] for feed in mydata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop my own tokenizer\n",
    "def tokenize_stories_lemma(story):\n",
    "    tokens = nltk.word_tokenize(story)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.replace(\"'s\", \" \").replace(\"n't\", \" not\").replace(\"'ve\", \" have\")\n",
    "        token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
    "        if token not in stopwords.words('english'):\n",
    "            filtered_tokens.append(token.lower())\n",
    "    lemmas = [lmtzr.lemmatize(t, 'v') for t in filtered_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing to be done to identify best set of parameters for min_df, max_df and the best number of topics. Best model is then applied to articles in each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using tfidf\n",
    "### Create tokenizer based on parameters\n",
    "def create_tfidf_tokenizer(min_df, max_df, ngram, max_features):\n",
    "    tf_vectorizer = TfidfVectorizer(min_df=min_df, \n",
    "                                    max_df=max_df, \n",
    "                                    ngram_range=(ngram, ngram), \n",
    "                                    tokenizer=tokenize_stories_lemma, \n",
    "                                    max_features=max_features)\n",
    "    return tf_vectorizer\n",
    "\n",
    "\n",
    "### Create tokenizer based on variable ngrams\n",
    "def create_flexible_tfidf_tokenizer(min_df, max_df, ngram, max_features):\n",
    "    tf_vectorizer = TfidfVectorizer(min_df=min_df, \n",
    "                                    max_df=max_df, \n",
    "                                    ngram_range=ngram, \n",
    "                                    tokenizer=tokenize_stories_lemma, \n",
    "                                    max_features=max_features)\n",
    "    return tf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply LSA to tfidf-vectorized text\n",
    "def apply_LSA(n_components, tfidf_docs):\n",
    "    svd = TruncatedSVD(n_components=n_components, n_iter=10)\n",
    "    svd_topic_vectors = svd.fit_transform(tfidf_docs)\n",
    "    return [svd, svd_topic_vectors]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to build LSA model \n",
    "def build_LSA_model(min_df, max_df, ngram, n_components, max_features, text_data, top_words=3):\n",
    "    print('Building tokenizer...')\n",
    "    \n",
    "    if isinstance(ngram, int):\n",
    "        tf_vectorizer = create_tfidf_tokenizer(min_df=min_df, max_df=max_df, ngram=ngram, max_features=max_features)\n",
    "    else:\n",
    "        tf_vectorizer = create_flexible_tfidf_tokenizer(min_df=min_df, max_df=max_df, ngram=ngram, max_features=max_features)\n",
    "    tfidf_docs = tf_vectorizer.fit_transform(text_data)\n",
    "    print('==================================================================')\n",
    "    \n",
    "    print('Generating LSA model outputs...')\n",
    "    svd, svd_topic_vectors = apply_LSA(n_components=n_components, tfidf_docs=tfidf_docs)\n",
    "    print('==================================================================')\n",
    "    \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    print('Generating topics...')\n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(svd.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-top_words-1:-1]]\n",
    "        print('Topic ' + str(topic_idx))\n",
    "        print(\" | \".join(tf_feature_names[i] for i in topic.argsort()[:-top_words-1:-1]))\n",
    "    \n",
    "    print('==================================================================')\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LSA model for: 2 topics and max_features: 20\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " airlines |  american |   \n",
      "Topic 1\n",
      "   | southwest airlines |  get\n",
      "==================================================================\n",
      "Process completed in 7.8181107950000035 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 2 topics and max_features: 50\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |  american |  airlines\n",
      "Topic 1\n",
      "   | credit card |  stock\n",
      "==================================================================\n",
      "Process completed in 7.881262741666664 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 2 topics and max_features: 100\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |  airlines |  american\n",
      "Topic 1\n",
      "   |   not | right \n",
      "==================================================================\n",
      "Process completed in 8.059177126666661 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 20\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " airlines |  american |   \n",
      "Topic 1\n",
      "   | southwest airlines |  get\n",
      "Topic 2\n",
      " get | time  |  one\n",
      "==================================================================\n",
      "Process completed in 7.613548364999997 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 50\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |  american |  airlines\n",
      "Topic 1\n",
      "   | credit card |  stock\n",
      "Topic 2\n",
      " get |  go |  one\n",
      "==================================================================\n",
      "Process completed in 7.364663015000004 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for: 3 topics and max_features: 100\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |  airlines |  american\n",
      "Topic 1\n",
      "   |   not | right \n",
      "Topic 2\n",
      "  not | fly  |  people\n",
      "==================================================================\n",
      "Process completed in 7.345298148333328 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for min_df: 100 and max_df: 1000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |  american |  airlines\n",
      "Topic 1\n",
      "   | credit card |  stock\n",
      "Topic 2\n",
      " get |  go |  one\n",
      "==================================================================\n",
      "Process completed in 7.311624175000005 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for min_df: 100 and max_df: 5000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "  | airlines  |  say\n",
      "Topic 1\n",
      "wear mask | mask  |   \n",
      "Topic 2\n",
      "airlines  | american airlines | flight \n",
      "==================================================================\n",
      "Process completed in 7.368646959999993 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for min_df: 500 and max_df: 1000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      " american |  airlines |   \n",
      "Topic 1\n",
      "   |   not |  take\n",
      "Topic 2\n",
      "  not | fly  |  get\n",
      "==================================================================\n",
      "Process completed in 7.296270904999998 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for min_df: 500 and max_df: 5000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "  | airlines  |  say\n",
      "Topic 1\n",
      "wear mask | mask  |   \n",
      "Topic 2\n",
      "airlines  | american airlines | flight \n",
      "==================================================================\n",
      "Process completed in 7.29425408666666 mins\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TESTING LSA MODEL\n",
    "\n",
    "# Optimize for topics and number of features\n",
    "n_topics = [2, 3]\n",
    "features = [20, 50, 100]\n",
    "for topic in n_topics:\n",
    "    for feature in features:\n",
    "        try:\n",
    "            tic = time.perf_counter()\n",
    "            print('Applying LSA model for: {0} topics and max_features: {1}'.format(topic, feature))\n",
    "            result = build_LSA_model(min_df=100, max_df=1000, ngram=2, n_components=topic, max_features=feature, text_data=stories)\n",
    "            toc = time.perf_counter()\n",
    "            print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "            print('==================================================================')\n",
    "            print()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# 3 topics, 50 features seems good. Optimize for min and max df:\n",
    "min_df = [100, 500]\n",
    "max_df = [1000, 5000]\n",
    "for mini in min_df:\n",
    "    for maxi in max_df:\n",
    "        try:\n",
    "            tic = time.perf_counter()\n",
    "            print('Applying LSA model for min_df: {0} and max_df: {1}'.format(mini, maxi))\n",
    "            result = build_LSA_model(min_df=mini, max_df=maxi, ngram=2, n_components=3, max_features=50, text_data=stories)\n",
    "            toc = time.perf_counter()\n",
    "            print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "            print('==================================================================')\n",
    "            print()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LSA model for max_df: 6000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "  | airlines  |  say\n",
      "Topic 1\n",
      "wear mask | mask  |   \n",
      "Topic 2\n",
      "airlines  | american airlines | flight \n",
      "==================================================================\n",
      "Process completed in 7.330493481666675 mins\n",
      "==================================================================\n",
      "\n",
      "Applying LSA model for max_df: 7000\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "  | airlines  |  say\n",
      "Topic 1\n",
      "wear mask | mask  |   \n",
      "Topic 2\n",
      "airlines  | american airlines | flight \n",
      "==================================================================\n",
      "Process completed in 7.304339103333344 mins\n",
      "==================================================================\n",
      "\n",
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "   |   say | say   | wear mask \n",
      "Topic 1\n",
      "wear mask  |  wear mask | refuse wear mask | mask  \n",
      "==================================================================\n",
      "Process completed in 7.336434568333334 mins\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "### min_df 100 is too low - use 500\n",
    "### max df not bad, maybe can test a bit higher\n",
    "\n",
    "max_df = [6000, 7000]\n",
    "for maxi in max_df:\n",
    "    tic = time.perf_counter()\n",
    "    print('Applying LSA model for max_df: {}'.format(maxi))\n",
    "    result = build_LSA_model(min_df=500, max_df=maxi, ngram=2, n_components=3, max_features=50, text_data=stories)\n",
    "    toc = time.perf_counter()\n",
    "    print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "    print('==================================================================')\n",
    "    print()\n",
    "\n",
    "\n",
    "### Looks better - let's test some more for top words and number of topics\n",
    "tic = time.perf_counter()\n",
    "result = build_LSA_model(min_df=100, max_df=8000, ngram=3, n_components=2, max_features=300, text_data=stories, top_words=4)\n",
    "toc = time.perf_counter()\n",
    "print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "print('==================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer...\n",
      "==================================================================\n",
      "Generating LSA model outputs...\n",
      "==================================================================\n",
      "Generating topics...\n",
      "Topic 0\n",
      "  | airlines |    | mask | flight\n",
      "Topic 1\n",
      "mask | wear | wear mask | mask  | wear mask \n",
      "==================================================================\n",
      "Process completed in 7.374587050000006 mins\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "### Let's try a mix of grams '''\n",
    "tic = time.perf_counter()\n",
    "result = build_LSA_model(min_df=10, max_df=8000, ngram=(1, 3), n_components=2, max_features=3000, text_data=stories, top_words=5)\n",
    "toc = time.perf_counter()\n",
    "print('Process completed in {} mins'.format((toc-tic)/60))\n",
    "print('==================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieving the top 5 articles titles for each topic \n",
    "\n",
    "### Generate a list of titles\n",
    "titles = [feed['title'] for feed in mydata]\n",
    "\n",
    "### Build a taxonomy to return top article titles from LSA model\n",
    "topic_taxonomy = {\n",
    "    'COVID': {\n",
    "        'COVID prevention protocol': 'wear mask'\n",
    "    },\n",
    "    'Airline companies': {\n",
    "        'Companies': 'american airlines united airlines southwest airlines',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving lists of keywords, labels and topics\n",
    "keyword_list = []\n",
    "label_list = []\n",
    "topic_list = []\n",
    "for key, value in topic_taxonomy.items():\n",
    "    topic_list.append(key)\n",
    "    for label, keywords in value.items():\n",
    "        keyword_list.append(keywords.lower())\n",
    "        label_list.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Finished loading Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "### Use word2vec to calculate similarities\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "model_word2vec = load_wordvec_model('Word2Vec', r'C:/Users/tramh/github/Data-Science-Portfolio/Airlines Covid-19/data/GoogleNews-vectors-negative300.bin.gz', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate vector similarity between two inputs\n",
    "def vec_similarity(input1, input2, vectors):\n",
    "    term_vectors = [np.zeros(300), np.zeros(300)]\n",
    "    terms = [input1, input2]\n",
    "        \n",
    "    for index, term in enumerate(terms):\n",
    "        for i, t in enumerate(term.split(' ')):\n",
    "            try:\n",
    "                term_vectors[index] += vectors[t]\n",
    "            except:\n",
    "                term_vectors[index] += 0\n",
    "        \n",
    "    result = (1 - spatial.distance.cosine(term_vectors[0], term_vectors[1]))\n",
    "    if result is 'nan':\n",
    "        result = 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and return top 3 articles for each item in topic_taxonomy\n",
    "def classify_topics(input, vectors):\n",
    "    feed_score = dict()\n",
    "    for key, value in topic_taxonomy.items():\n",
    "        max_value_score = dict()\n",
    "        for label, keywords in value.items():\n",
    "            max_value_score[label] = 0\n",
    "            topic = (key + ' ' + keywords).strip()\n",
    "            max_value_score[label] += float(calc_similarity(input, topic, vectors))\n",
    "            \n",
    "        sorted_max_score = sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "        feed_score[sorted_max_score[0]] = sorted_max_score[1]\n",
    "    return sorted(feed_score.items(), key=operator.itemgetter(1), reverse=True)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate topic classifications into a dictionary - key is title index, value is the classification topics and their similarity scores\n",
    "results = {}\n",
    "for i in range(len(titles)):\n",
    "    try:\n",
    "        results[i] = classify_topics(titles[i], model_word2vec)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "''' Mapping topics to Top 5 Articles '''\n",
    "# For given topic, return dictionary of article index (key) mapped to similarity score (value)\n",
    "def extract_top_articles_from_topic(topic, label, classified_topics):\n",
    "    topic_articles = {}\n",
    "    for feed_idx, labels in classified_topics.items():\n",
    "        for label_value_tuple in labels:\n",
    "            if label_value_tuple[0] == label:\n",
    "                topic_articles[feed_idx] = label_value_tuple[1]\n",
    "    # Return just top 10 articles\n",
    "    sorted_topic_articles = {k: v for k, v in sorted(topic_articles.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # Return just top 5 articles\n",
    "    return list(sorted_topic_articles.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map topic articles to actual article titles\n",
    "def get_article_titles(top_topics, titles):\n",
    "    articles_to_similarity = {}\n",
    "    for feed in top_topics:\n",
    "        articles_to_similarity[feed[0]] = [titles[feed[0]], feed[1]]\n",
    "    return articles_to_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: COVID \n",
      " \n",
      "Label: COVID prevention protocol \n",
      " \n",
      "Top 5 articles and similarity scores \n",
      "\n",
      "Alaska Airlines: Wear mask or else! 0.68\n",
      "Airline mask policy: Do you have to wear a mask on a plane? 0.66\n",
      "Coronavirus: United Airlines says you need to wear face mask in airport 0.6\n",
      "Airlines are actually banning flyers who won’t wear masks 0.58\n",
      "Alaska Airlines could suspend passengers who refuse to wear a mask 0.57\n",
      "\n",
      "=========================================================================================\n",
      "Topic: Airline companies \n",
      " \n",
      "Label: Companies \n",
      " \n",
      "Top 5 articles and similarity scores \n",
      "\n",
      "southwest philly airlines 0.75\n",
      "united airlines to require masks at all airports 0.7\n",
      "Information for american airlines cancellation policy 0.68\n",
      "TC-JND A330-200 Turkish airlines 17-08-16 ebbr maarten-sr 0.68\n",
      "eight airlines pinned, they “violate passenger rights” 0.67\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reverse the taxonomy\n",
    "reverse_taxonomy = {}\n",
    "for topic, label_dict in topic_taxonomy.items():\n",
    "    for label, keywords in label_dict.items():\n",
    "        reverse_taxonomy[label] = topic\n",
    "\n",
    "\n",
    "for label, topic in reverse_taxonomy.items():\n",
    "    print('Topic: {} \\n '.format(topic))\n",
    "    print('Label: {} \\n '.format(label))\n",
    "    print('Top 5 articles and similarity scores \\n')\n",
    "    top_articles = extract_top_articles_from_topic(topic=topic, label=label, classified_topics=results)\n",
    "    final_results = get_article_titles(top_articles, titles)\n",
    "    for result in final_results:\n",
    "        print(final_results[result][0], round(final_results[result][1], 2))\n",
    "    print()\n",
    "    print('=========================================================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
